{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa230e8a-8e9c-435c-bc9f-291f3a90b07a",
   "metadata": {},
   "source": [
    "# Analyze data and LLM answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ec2427-702e-4a47-8477-172e0568759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gat_llm.llm_invoker as inv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import medqa.judge\n",
    "from medqa.datasplit import get_split\n",
    "from medqa.qa_retrieval_agent import QARetrievalAgent\n",
    "from medqa.glove_retriever import GloVeSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4f8c4-d2a6-4beb-9a2b-11fc3f6fce11",
   "metadata": {},
   "source": [
    "## Create LLMs to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a8c54-2b4a-46ee-b761-461de230430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"Qwen 3 1.7b - Ollama\"\n",
    "llm = inv.LLM_Provider.get_llm(None, llm_name)\n",
    "\n",
    "medqa_prompt = \"\"\"You are medical question-answering system expert at providing medical information.\n",
    "Your goal is to effectively answer user queries related to medical diseases.\n",
    "Refuse to answer questions not related to your expertise.\"\"\"\n",
    "\n",
    "judge_llm = medqa.judge.AnswerJudge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d79ac-d1d0-4ad9-ac51-a615c35974db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80368245-b548-49cb-b307-8acdf597c9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aac6c11b-2c2a-4e46-8c0b-26cd1089be77",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ae214-3410-41ff-acc8-2750b1389e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/intern_screening_dataset.csv\"\n",
    "df = pd.read_csv(file)\n",
    "row_split = [get_split(x) for x in list(df.index)]\n",
    "df[\"split\"] = row_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23abe08-3add-4935-a94f-23849b4f6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a2439-a7b9-487f-ba71-da2a3d5835ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.split == \"train\"][\"question\"].to_csv(\"data/train_questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f6b6a-a4d2-482f-8ec2-c7505c90f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "qara = QARetrievalAgent(df[df.split == \"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8531c-091c-4f4b-adb0-f059c91e31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"temp.json\", \"w\") as f:\n",
    "#    f.write(json.dumps(qara.questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd22151-97e8-4e8e-976a-61978a969d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df[df.split == \"val\"].reset_index(drop=True)\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f091c-e41e-4a36-9343-fe116e981a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_agg = df_val.groupby('question')['answer'].apply(list).reset_index()\n",
    "df_val_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca321802-b299-4df0-98fb-410f5b11f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_agg.iloc[10].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efaea1-7bd7-4efc-9c8e-e6ebaeaf82f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d063754-9859-49e6-8096-e45640c56ad7",
   "metadata": {},
   "source": [
    "## Assign scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f39104-c035-4d7a-b853-7386c67c99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val_agg = df_val_agg[0:5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d7249-5c7b-4d2b-8ef4-48b2433538c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all_metrics = []\n",
    "all_reasoning = []\n",
    "all_ans = []\n",
    "for idx, row in tqdm(df_val_agg.iterrows(), total=len(df_val_agg)):\n",
    "    ans = llm(\n",
    "        row[\"question\"],\n",
    "        chat_history=[],\n",
    "        system_prompt=medqa_prompt,\n",
    "    )\n",
    "    for x in ans:\n",
    "        pass\n",
    "    candidate_ans = x\n",
    "    if \"</think>\" in candidate_ans:\n",
    "        candidate_ans = candidate_ans.split(\"</think>\")[-1]\n",
    "    reasoning, metrics = judge_llm.judge_answer(row[\"question\"], row[\"answer\"], candidate_ans)\n",
    "    all_ans.append(candidate_ans)\n",
    "    all_metrics.append(metrics)\n",
    "    all_reasoning.append(reasoning)\n",
    "\n",
    "df_val_agg[\"candidate_ans\"] = all_ans\n",
    "df_val_agg[\"metrics\"] = all_metrics\n",
    "df_val_agg[\"reasoning\"] = all_reasoning\n",
    "\n",
    "df_val_agg.to_csv(\"zero_shot_metrics.csv\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6077c-1bef-493a-89ed-a17ed96144ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a726f0-abd5-45b0-8093-68d2db857b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_val_agg.iloc[10]\n",
    "row[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0046325-2abc-4ec4-9869-1400679c9299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5376d5b-cb22-4dee-92b5-620a6b7753e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c21236-f001-444f-a1da-4656c48fba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val_agg.to_csv(\"zero_shot_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac631e03-398b-49ee-98be-c990baadc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df_val_agg = pd.read_csv(\"metrics/zero_shot_metrics.csv\")\n",
    "df_val_agg.metrics = df_val_agg.metrics.map(ast.literal_eval)\n",
    "df_val_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc80577-c94c-4f64-a55d-6d65387f6195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d3de2-e1c3-49ce-bb18-0ca1824ac3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02686fc9-aa34-45ef-82ed-e2fdefd45a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f602579-df0a-489a-8925-7db3a94080b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67e04a16-321b-496c-8529-3f2b3f0a83f9",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7880d1-d025-4b92-a912-0d250f69933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = llm(\n",
    "    row[\"question\"],\n",
    "    chat_history=[],\n",
    "    system_prompt=medqa_prompt,\n",
    ")\n",
    "prev = \"\"\n",
    "for x in ans:\n",
    "    cur_ans = x\n",
    "    # print(cur_ans.replace(prev, ''))\n",
    "    # prev = cur_ans\n",
    "    print('.', end='')\n",
    "print('\\n')\n",
    "print(x)\n",
    "candidate_ans = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892acaf-64de-4c02-b897-8e225dfcd2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"</think>\" in candidate_ans:\n",
    "    candidate_ans = candidate_ans.split(\"</think>\")[-1]\n",
    "reasoning, metrics = judge_llm.judge_answer(row[\"question\"], row[\"answer\"], candidate_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660562bc-0bda-4c68-ae79-def49c499cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[\"question\"], row[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f655f-919e-4851-a2c7-ca19d836fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26667c68-80d8-458c-855d-b29539c3f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c70319-bd62-4e72-80f7-9766265ddc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "qara = QARetrievalAgent(df[df.split == \"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8d2e3-ce9a-43b3-8daa-17568afb158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_q = qara(\"What is (are) diabetes\")\n",
    "print(sel_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27710ca-3f6f-4df8-a6ed-b40688b7dfed",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353976ed-e7ef-4b7f-b0fe-06b954afeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GloVeSimilarity(df[df.split == \"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3f6db-dc89-4f4a-a9ef-db42e30ba820",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = gs.find_documents(\"Tell me about weaver syndrome\")\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddd96f-a3bf-4f0d-aba3-62c17bf8b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.iloc[4].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ab448-182f-4831-ae6d-a4bc69903cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs._get_sentence_emb(\"What is (are) Glaucoma ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1e375-11e1-4de5-9ed2-05c89823c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs(\"What is (are) Glaucoma ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5df1b-a22f-4e2a-94f6-a8b325b90d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
