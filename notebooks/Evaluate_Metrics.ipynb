{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4626a235-ebd7-441e-a04f-2383f00be82f",
   "metadata": {},
   "source": [
    "# Open inference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48667ae-3df4-49e4-bc88-491d32643f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from medqa.classical_metrics import get_bleu_score\n",
    "from medqa.classical_metrics import get_rouge_scores\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module='nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26556ed-a08c-4d6c-9ba4-6b997f2f9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_files = os.listdir(\"metrics\")\n",
    "metrics_files = [os.path.join(\"metrics\", x) for x in metrics_files]\n",
    "metrics_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc4508-02b2-49cd-a819-77bc7e2d7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nlp_metrics(df):\n",
    "    \"\"\" Receives a dataframe with \"answer\" and \"candidate_answer\"\n",
    "    and computes BLEU and rouge scores\n",
    "    \"\"\"\n",
    "    scores = {\"LLM_judge\": [], \"bleu\": [], \"rouge1\": [], 'rouge2': [], 'rougeL': []}\n",
    "    # judge metrics\n",
    "    for idx, row in df.iterrows():\n",
    "        eval_string = row[\"metrics\"]\n",
    "\n",
    "        # since the LLM is small, sometimes it forgets the brackets []\n",
    "        if not eval_string.startswith(\"[\"):\n",
    "            eval_string = f\"[{eval_string}]\"\n",
    "        judge_score = ast.literal_eval(eval_string)\n",
    "\n",
    "        # since the LLM is small, sometimes the LLM judge assigns score > 1\n",
    "        judge_score = min(1.0, np.sum(judge_score) / 6.0)\n",
    "        scores[\"LLM_judge\"].append(judge_score)\n",
    "\n",
    "    # BLEU and rouge\n",
    "    for idx, row in tqdm(df.iterrows(), desc=\"Computing scores\", total=len(df)):\n",
    "        ref_answers = ast.literal_eval(row[\"answer\"])\n",
    "        cand_answer = row[\"candidate_ans\"]\n",
    "        bleu_score = get_bleu_score(ref_answers, cand_answer)\n",
    "        scores[\"bleu\"].append(bleu_score)\n",
    "        rouge_scores = get_rouge_scores(ref_answers, cand_answer)\n",
    "        scores[\"rouge1\"].append(rouge_scores[0])\n",
    "        scores[\"rouge2\"].append(rouge_scores[1])\n",
    "        scores[\"rougeL\"].append(rouge_scores[2])\n",
    "    report = {\n",
    "        \"number_of_qa_pairs\": len(df),\n",
    "        \"LLM_judge_avg\": str(np.mean(scores[\"LLM_judge\"])) + \" N/A\",\n",
    "        \"LLM_judge_std\": str(np.std(scores[\"LLM_judge\"])) + \" N/A\",\n",
    "        \"bleu_avg\": np.mean(scores[\"bleu\"]),\n",
    "        \"bleu_std\": np.std(scores[\"bleu\"]),\n",
    "        \"rouge1_avg\": np.mean(scores[\"rouge1\"]),\n",
    "        \"rouge1_std\": np.std(scores[\"rouge1\"]),\n",
    "        \"rouge2_avg\": np.mean(scores[\"rouge2\"]),\n",
    "        \"rouge2_std\": np.std(scores[\"rouge2\"]),\n",
    "        \"rougeL_avg\": np.mean(scores[\"rougeL\"]),\n",
    "        \"rougeL_std\": np.std(scores[\"rougeL\"]),\n",
    "    }\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83059306-1873-4d3a-857c-810b947096b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "for file in metrics_files:\n",
    "    df = pd.read_csv(file)\n",
    "    metrics = compute_nlp_metrics(df)\n",
    "    metrics[\"file\"] = file\n",
    "    all_metrics.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f472d1d-5fd5-4f75-b7bb-56e485106563",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_metrics).set_index(\"file\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa263d-cee3-4c85-bfc4-3480f70e3aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a4555-b1ff-4e10-b2d9-8a36f7f9ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ff05d-a819-4fd7-8e9c-58b85e8fb133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
